{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Rapport individuel\n",
    "Chaque √©tudiant doit r√©diger un rapport individuel (10 √† 20 pages maximum), structur√© comme suit :\n",
    "\n",
    "## a. D√©finition du probl√®me :\n",
    "- Identifiez la probl√©matique m√©tier √† r√©soudre.\n",
    "- Pr√©cisez les objectifs et les exigences du projet.\n",
    "\n",
    "## b. Collecte des donn√©es :\n",
    "- Choisissez un dataset pertinent pour votre projet.\n",
    "- √âvaluez la qualit√© et la quantit√© des donn√©es disponibles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---  \n",
    "\n",
    "```md\n",
    "# üìä Analyse Exploratoire d'un Dataset CSV avec Pandas, NumPy, Seaborn et Matplotlib\n",
    "\n",
    "Ce script Python permet d'effectuer une **analyse exploratoire des donn√©es (EDA)** sur un fichier **CSV** en utilisant les biblioth√®ques **Pandas, NumPy, Seaborn et Matplotlib**. Il fournit un aper√ßu des donn√©es, identifie les valeurs manquantes et dupliqu√©es, affiche des statistiques descriptives et g√©n√®re des visualisations utiles pour comprendre la distribution des donn√©es et leur corr√©lation.\n",
    "\n",
    "## üìå Pr√©requis\n",
    "\n",
    "Avant d'ex√©cuter ce script, assurez-vous d'avoir install√© les biblioth√®ques n√©cessaires avec la commande suivante :\n",
    "\n",
    "```bash\n",
    "pip install pandas numpy seaborn matplotlib\n",
    "```\n",
    "\n",
    "## üìÇ Chargement du Dataset\n",
    "\n",
    "Le script commence par charger un fichier CSV nomm√© **\"02-14-2018-2.csv\"** dans un DataFrame Pandas :\n",
    "\n",
    "```python\n",
    "file_path = \"02-14-2018-2.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "```\n",
    "\n",
    "Si le fichier n'est pas dans le m√™me r√©pertoire que le script, vous devrez ajuster le chemin du fichier.\n",
    "\n",
    "## üîç 1. Aper√ßu des Donn√©es\n",
    "\n",
    "Les premi√®res lignes du dataset sont affich√©es pour donner un premier aper√ßu du contenu :\n",
    "\n",
    "```python\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "Cela permet d'identifier rapidement les colonnes et le type de donn√©es pr√©sentes.\n",
    "\n",
    "## üìÉ 2. Informations G√©n√©rales sur le Dataset\n",
    "\n",
    "Le script affiche des informations d√©taill√©es sur le dataset, y compris le **nombre total de lignes et de colonnes**, ainsi que les **types de donn√©es** :\n",
    "\n",
    "```python\n",
    "print(df.info())\n",
    "```\n",
    "\n",
    "Ce rapport permet de d√©tecter si certaines colonnes contiennent des valeurs **nulles** ou des **types de donn√©es incoh√©rents**.\n",
    "\n",
    "## üî¢ 3. Nombre de Lignes et Colonnes\n",
    "\n",
    "Le nombre total de **lignes** et **colonnes** du dataset est affich√© :\n",
    "\n",
    "```python\n",
    "print(f\"Nombre de lignes : {df.shape[0]}  |  Nombre de colonnes : {df.shape[1]}\")\n",
    "```\n",
    "\n",
    "Cela donne une id√©e g√©n√©rale de la **taille du dataset**.\n",
    "\n",
    "## ‚ö†Ô∏è 4. V√©rification des Valeurs Manquantes\n",
    "\n",
    "Les valeurs manquantes sont identifi√©es en calculant le nombre de **valeurs nulles par colonne** :\n",
    "\n",
    "```python\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "print(missing_values)\n",
    "```\n",
    "\n",
    "Seules les colonnes contenant **des valeurs manquantes** sont affich√©es. Cela permet d'√©valuer si un **nettoyage des donn√©es** est n√©cessaire.\n",
    "\n",
    "## üìä 5. Distribution des Classes (Trafic Normal vs Attaques)\n",
    "\n",
    "Une **visualisation** de la r√©partition des labels dans le dataset est g√©n√©r√©e avec Seaborn :\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(y=df['Label'], order=df['Label'].value_counts().index)\n",
    "plt.title(\"üìä Distribution des attaques et du trafic normal\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "- La colonne `Label` est suppos√©e contenir des **cat√©gories de trafic** (par ex. \"normal\" vs. \"attaque\").\n",
    "- Un **graphique en barres** est utilis√© pour montrer le **nombre d'occurrences** de chaque type de trafic.\n",
    "\n",
    "## üìà 6. Statistiques Descriptives des Variables Num√©riques\n",
    "\n",
    "Le script affiche un **r√©sum√© statistique** des colonnes num√©riques :\n",
    "\n",
    "```python\n",
    "print(df.describe())\n",
    "```\n",
    "\n",
    "Cela inclut :\n",
    "- **Moyenne (mean)**\n",
    "- **√âcart-type (std)**\n",
    "- **Valeurs minimales et maximales**\n",
    "- **Quartiles (25%, 50%, 75%)**\n",
    "\n",
    "Ces informations sont essentielles pour d√©tecter **d'√©ventuelles anomalies** ou valeurs aberrantes.\n",
    "\n",
    "## üîÑ 7. V√©rification des Valeurs Dupliqu√©es\n",
    "\n",
    "Le script d√©tecte et affiche le nombre de **lignes dupliqu√©es** dans le dataset :\n",
    "\n",
    "```python\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Nombre de lignes dupliqu√©es : {duplicates}\")\n",
    "```\n",
    "\n",
    "Si des doublons sont d√©tect√©s, il peut √™tre utile de les supprimer pour √©viter des biais dans l'analyse.\n",
    "\n",
    "## üî¨ 8. Matrice de Corr√©lation des 10 Premi√®res Variables\n",
    "\n",
    "Une **matrice de corr√©lation** est g√©n√©r√©e pour analyser la **relation entre les variables num√©riques** :\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(12, 8))\n",
    "corr_matrix = df.select_dtypes(include=[np.number]).corr()\n",
    "sns.heatmap(corr_matrix.iloc[:10, :10], annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"üîç Matrice de Corr√©lation (10 premi√®res variables)\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "- Seules les **10 premi√®res colonnes** num√©riques sont affich√©es.\n",
    "- Une **carte thermique (heatmap)** est utilis√©e pour mieux visualiser les corr√©lations.\n",
    "- Une **forte corr√©lation** (valeurs proches de **+1** ou **-1**) peut indiquer une **redondance** entre certaines variables.\n",
    "\n",
    "## ‚úÖ Conclusion\n",
    "\n",
    "√Ä la fin du script, un message de confirmation est affich√© :\n",
    "\n",
    "```python\n",
    "print(\"\\n‚úÖ √âvaluation des donn√©es termin√©e !\")\n",
    "```\n",
    "\n",
    "Ce script fournit une **analyse rapide et efficace** d'un dataset CSV, facilitant la **pr√©paration des donn√©es** avant une analyse plus approfondie ou un entra√Ænement de mod√®le machine learning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import matplotlib.font_manager as fm\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "\n",
    "# ‚úÖ Define a compatible font to avoid display errors\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'  # Replaces Noto Sans\n",
    "\n",
    "# ‚úÖ Adjust display parameters to avoid truncation\n",
    "pd.options.display.max_rows = 50  # Adjust as needed\n",
    "pd.options.display.max_columns = 20\n",
    "\n",
    "# üìå Load a CSV file (adjust path according to file location)\n",
    "file_path = \"02-14-2018-2.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# ‚úÖ Checking and replacing infinite values\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# üîπ 1. Dataset overview\n",
    "print(\"\\nAper√ßu des premi√®res lignes du dataset :\")\n",
    "print(df.head())\n",
    "\n",
    "# üîπ 2. General dataset information\n",
    "print(\"\\nInformations g√©n√©rales :\")\n",
    "print(df.info())\n",
    "\n",
    "# üîπ 3. Number of rows and columns\n",
    "print(f\"\\nNombre de lignes : {df.shape[0]}  |  Nombre de colonnes : {df.shape[1]}\")\n",
    "\n",
    "# üîπ 4. Checking for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]  # Keep only columns with zero values\n",
    "if not missing_values.empty:\n",
    "    print(\"\\nColonnes avec valeurs manquantes :\")\n",
    "    print(missing_values)\n",
    "else:\n",
    "    print(\"\\nAucune valeur manquante d√©tect√©e.\")\n",
    "\n",
    "# üîπ 5. Class distribution (normal traffic vs. attacks)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(y=df['Label'], order=df['Label'].value_counts().index)\n",
    "plt.title(\"Distribution des attaques et du trafic normal\")\n",
    "plt.show()\n",
    "\n",
    "# üîπ 6. Descriptive statistics for numerical variables\n",
    "print(\"\\nCalcul des statistiques descriptives en cours...\")\n",
    "stats_desc = df.describe()\n",
    "\n",
    "# ‚úÖ Save to file to avoid truncated display\n",
    "stats_desc.to_csv(\"statistiques_descriptives.csv\")\n",
    "print(\"\\nStatistiques descriptives enregistr√©es dans 'statistiques_descriptives.csv'. Ouvrez ce fichier pour voir toutes les valeurs.\")\n",
    "\n",
    "# ‚úÖ Partial display to avoid truncation in the console\n",
    "print(stats_desc.iloc[:, :10])  # Display first 10 columns only\n",
    "\n",
    "# üîπ 7. Checking duplicate values\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nNombre de lignes dupliqu√©es : {duplicates}\")\n",
    "\n",
    "# üîπ 8. Correlation matrix for the first 10 variables\n",
    "plt.figure(figsize=(12, 8))\n",
    "corr_matrix = df.select_dtypes(include=[np.number]).corr()\n",
    "sns.heatmap(corr_matrix.iloc[:10, :10], annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Matrice de Corr√©lation (10 premi√®res variables)\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n√âvaluation des donn√©es termin√©e !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c. Nettoyage et pr√©paration des donn√©es :\n",
    "- Effectuez le nettoyage des donn√©es (gestion des doublons, traitement des valeurs manquantes, etc.).\n",
    "- Transformez et formatez les donn√©es si n√©cessaire.\n",
    "- Fusionnez diff√©rentes sources de donn√©es, le cas √©ch√©ant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üõ†Ô∏è Pr√©traitement des Donn√©es avec Pandas, NumPy et Scikit-Learn\n",
    "\n",
    "Ce script effectue un **pr√©traitement avanc√© des donn√©es** sur un dataset charg√© avec **Pandas**. Il supprime les doublons, g√®re les valeurs manquantes et aberrantes, convertit les types de donn√©es, encode les labels et normalise les features avant de sauvegarder le dataset nettoy√©.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Pr√©requis\n",
    "\n",
    "Assurez-vous d'avoir install√© les biblioth√®ques n√©cessaires avec la commande :\n",
    "\n",
    "```bash\n",
    "pip install pandas numpy seaborn matplotlib scikit-learn\n",
    "```\n",
    "\n",
    "## üóÇÔ∏è Chargement des Donn√©es\n",
    "\n",
    "Le script suppose que les donn√©es sont d√©j√† charg√©es dans un DataFrame `df`. Si ce n'est pas le cas, vous pouvez ajouter :\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(\"02-14-2018-2.csv\")  \n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 1. Suppression des Doublons\n",
    "\n",
    "Les doublons sont d√©tect√©s et supprim√©s pour √©viter la **redondance des donn√©es** :\n",
    "\n",
    "```python\n",
    "print(f\"Nombre de doublons avant suppression : {df.duplicated().sum()}\")\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(f\"Nombre de doublons apr√®s suppression : {df.duplicated().sum()}\")\n",
    "```\n",
    "\n",
    "- **Avant suppression**, on affiche le **nombre de lignes dupliqu√©es**.\n",
    "- **Apr√®s suppression**, on v√©rifie qu'il ne reste plus de doublons.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 2. Traitement des Valeurs Manquantes\n",
    "\n",
    "Le script identifie et corrige les **valeurs nulles** dans le dataset.\n",
    "\n",
    "```python\n",
    "print(\"Valeurs manquantes avant traitement :\")\n",
    "print(df.isnull().sum()[df.isnull().sum() > 0])\n",
    "```\n",
    "\n",
    "Deux m√©thodes sont utilis√©es :\n",
    "1. **Remplacement par la m√©diane** pour les colonnes num√©riques :\n",
    "   ```python\n",
    "   for col in df.select_dtypes(include=[np.number]).columns:\n",
    "       df[col].fillna(df[col].median(), inplace=True)\n",
    "   ```\n",
    "   ‚Üí Cela √©vite de **biaiser les distributions** en conservant une tendance centrale.\n",
    "\n",
    "2. **Suppression des lignes restantes contenant des valeurs manquantes** :\n",
    "   ```python\n",
    "   df.dropna(inplace=True)\n",
    "   ```\n",
    "   ‚Üí Utile si certaines valeurs manquantes ne peuvent √™tre remplac√©es.\n",
    "\n",
    "Enfin, on v√©rifie que **toutes les valeurs nulles ont √©t√© supprim√©es** :\n",
    "\n",
    "```python\n",
    "print(f\"Valeurs manquantes apr√®s traitement : {df.isnull().sum().sum()}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 3. Gestion des Valeurs Infinies et Aberrantes\n",
    "\n",
    "Le script identifie les **valeurs infinies** qui peuvent perturber les analyses.\n",
    "\n",
    "```python\n",
    "print(\"V√©rification des valeurs infinies avant traitement :\")\n",
    "print(df.replace([np.inf, -np.inf], np.nan).isnull().sum()[df.replace([np.inf, -np.inf], np.nan).isnull().sum() > 0])\n",
    "```\n",
    "\n",
    "- Les valeurs **infinies** (`np.inf`, `-np.inf`) sont remplac√©es par **NaN** :\n",
    "  ```python\n",
    "  df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "  ```\n",
    "  ‚Üí Cela √©vite des erreurs lors des calculs et visualisations.\n",
    "\n",
    "- On v√©rifie ensuite les **valeurs extr√™mes** :\n",
    "  ```python\n",
    "  max_value = df.select_dtypes(include=[np.number]).max().max()\n",
    "  if max_value > 1e15:\n",
    "      df[df > 1e15] = np.nan\n",
    "      print(\"‚ö†Ô∏è Certaines valeurs √©taient trop grandes et ont √©t√© remplac√©es par NaN.\")\n",
    "  ```\n",
    "  ‚Üí Un seuil de **10¬π‚Åµ** est fix√© pour identifier les valeurs anormalement √©lev√©es.\n",
    "\n",
    "- Apr√®s cela, toutes les valeurs `NaN` restantes sont supprim√©es :\n",
    "\n",
    "  ```python\n",
    "  df.dropna(inplace=True)\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 4. V√©rification et Correction des Types de Donn√©es\n",
    "\n",
    "Le script affiche les types de donn√©es avant correction :\n",
    "\n",
    "```python\n",
    "print(\"Types de donn√©es avant transformation :\")\n",
    "print(df.dtypes)\n",
    "```\n",
    "\n",
    "- Il tente de **convertir les colonnes de type \"object\" en float** si possible :\n",
    "  ```python\n",
    "  for col in df.select_dtypes(include=['object']).columns:\n",
    "      try:\n",
    "          df[col] = df[col].astype(float)\n",
    "      except:\n",
    "          pass  # Certaines colonnes resteront du texte (ex: 'Label')\n",
    "  ```\n",
    "  ‚Üí Cela est utile pour garantir que les **colonnes num√©riques sont bien exploitables**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 5. Encodage de la Colonne `Label` (Attaque vs Normal)\n",
    "\n",
    "Si le dataset contient une colonne `Label` indiquant **le type de trafic**, celle-ci est transform√©e en **valeurs num√©riques** (`0` pour normal et `1` pour attaque).\n",
    "\n",
    "```python\n",
    "if 'Label' in df.columns:\n",
    "    le = LabelEncoder()\n",
    "    df['Label'] = le.fit_transform(df['Label'])\n",
    "```\n",
    "\n",
    "Apr√®s encodage, on affiche la distribution des labels :\n",
    "\n",
    "```python\n",
    "print(\"Distribution des labels apr√®s encodage :\")\n",
    "print(df['Label'].value_counts())\n",
    "```\n",
    "\n",
    "- Cette √©tape est cruciale pour **les mod√®les de Machine Learning**, qui ne traitent pas directement les cat√©gories textuelles.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 6. Normalisation des Features Num√©riques (Standardisation)\n",
    "\n",
    "Toutes les **features num√©riques** sont normalis√©es √† l'aide de la **standardisation** :\n",
    "\n",
    "```python\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "```\n",
    "\n",
    "- Cela transforme chaque **variable** pour qu'elle ait une **moyenne de 0** et un **√©cart-type de 1**.\n",
    "- Utile pour am√©liorer la **performance des mod√®les de classification**.\n",
    "\n",
    "Une confirmation est affich√©e :\n",
    "\n",
    "```python\n",
    "print(\"‚úÖ Normalisation des features termin√©e.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 7. Sauvegarde des Donn√©es Nettoy√©es\n",
    "\n",
    "Les donn√©es nettoy√©es sont enregistr√©es dans un **nouveau fichier CSV** :\n",
    "\n",
    "```python\n",
    "df.to_csv(\"cleaned_02-14-2018-2.csv\", index=False)\n",
    "```\n",
    "\n",
    "Une confirmation est affich√©e :\n",
    "\n",
    "```python\n",
    "print(\"‚úÖ Fichier cleaned_02-14-2018-2.csv enregistr√© avec succ√®s !\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Conclusion\n",
    "\n",
    "Ce script effectue un **nettoyage avanc√©** des donn√©es, essentiel avant toute analyse ou mod√©lisation. Il permet de :\n",
    "\n",
    "‚úîÔ∏è **Supprimer les doublons**  \n",
    "‚úîÔ∏è **G√©rer les valeurs manquantes et infinies**  \n",
    "‚úîÔ∏è **Corriger les types de donn√©es**  \n",
    "‚úîÔ∏è **Encoder les labels**  \n",
    "‚úîÔ∏è **Standardiser les donn√©es num√©riques**  \n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Am√©liorations Possibles\n",
    "\n",
    "Voici quelques pistes d'am√©lioration pour ce script :\n",
    "- **Ajouter une d√©tection automatique des valeurs aberrantes** avec des m√©thodes comme **IQR (Interquartile Range)**.\n",
    "- **G√©n√©rer des visualisations** pour identifier les **distributions** avant et apr√®s le traitement.\n",
    "- **Cr√©er une pipeline de pr√©traitement** avec `sklearn.pipeline`.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ R√©sum√© des √âtapes en Code\n",
    "\n",
    "```python\n",
    "# 1. Suppression des doublons\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# 2. Traitement des valeurs manquantes\n",
    "df.fillna(df.median(), inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# 3. Gestion des valeurs infinies et aberrantes\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# 4. Correction des types de donn√©es\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    try:\n",
    "        df[col] = df[col].astype(float)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# 5. Encodage des labels\n",
    "if 'Label' in df.columns:\n",
    "    df['Label'] = LabelEncoder().fit_transform(df['Label'])\n",
    "\n",
    "# 6. Normalisation\n",
    "df[df.select_dtypes(include=[np.number]).columns] = StandardScaler().fit_transform(df.select_dtypes(include=[np.number]))\n",
    "\n",
    "# 7. Sauvegarde des donn√©es nettoy√©es\n",
    "df.to_csv(\"cleaned_02-14-2018-2.csvsv\", index=False)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ 9. Deleting duplicates\n",
    "print(f\"\\nüîç Nombre de doublons avant suppression : {df.duplicated().sum()}\")\n",
    "df = df.drop_duplicates()  # ‚úÖ Suppression correcte sans inplace=True\n",
    "print(f\"‚úÖ Nombre de doublons apr√®s suppression : {df.duplicated().sum()}\")\n",
    "\n",
    "# ‚úÖ 10. Handling missing values\n",
    "print(\"\\nüîç Valeurs manquantes avant traitement :\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "print(missing_values)\n",
    "\n",
    "# Replacement of missing values by the median for numerical columns\n",
    "for col in df.select_dtypes(include=[np.number]).columns:\n",
    "    df.loc[:, col] = df[col].fillna(df[col].median())  # ‚úÖ Use .loc[] to avoid chained copies\n",
    "\n",
    "# Deletion of remaining lines with zero values\n",
    "df = df.dropna()\n",
    "print(\"\\n‚úÖ Valeurs manquantes apr√®s traitement :\", df.isnull().sum().sum())\n",
    "\n",
    "# ‚úÖ 11. Handling infinite values and outliers\n",
    "print(\"\\nüîç V√©rification des valeurs infinies avant traitement :\")\n",
    "infinite_values = df.replace([np.inf, -np.inf], np.nan).isnull().sum()\n",
    "infinite_values = infinite_values[infinite_values > 0]\n",
    "print(infinite_values)\n",
    "\n",
    "# Replacing infinite values with NaN\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "print(\"\\n‚úÖ Valeurs infinies remplac√©es par NaN.\")\n",
    "\n",
    "# Verify and remove extreme values (too large for dtype float64)\n",
    "max_value = df.select_dtypes(include=[np.number]).max().max()  # Find the largest numerical value\n",
    "if max_value > 1e15:  # Safety threshold to avoid extreme values\n",
    "    df.loc[:, df > 1e15] = np.nan\n",
    "    print(\"\\n‚ö†Ô∏è Certaines valeurs √©taient trop grandes et ont √©t√© remplac√©es par NaN.\")\n",
    "\n",
    "# Removal of remaining NaN after infinity conversion\n",
    "df = df.dropna()\n",
    "print(\"\\n‚úÖ Suppression des NaN apr√®s traitement des valeurs infinies et aberrantes.\")\n",
    "\n",
    "# ‚úÖ 12. DateTime column management\n",
    "print(\"\\nüîç D√©tection des colonnes datetime...\")\n",
    "datetime_cols = []\n",
    "\n",
    "for col in df.columns:\n",
    "    try:\n",
    "        df[col] = pd.to_datetime(df[col])  # Convert if date\n",
    "        datetime_cols.append(col)\n",
    "        print(f\"üìÖ Converti '{col}' en format DateTime.\")\n",
    "    except (ValueError, TypeError):\n",
    "        pass  # Ignore if conversion fails\n",
    "\n",
    "if datetime_cols:\n",
    "    # Option 1: Extract useful features\n",
    "    for col in datetime_cols:\n",
    "        df[col + '_hour'] = df[col].dt.hour\n",
    "        df[col + '_day'] = df[col].dt.day\n",
    "        df[col + '_month'] = df[col].dt.month\n",
    "        df[col + '_weekday'] = df[col].dt.weekday\n",
    "    \n",
    "    # Option 2: Delete original datetime columns\n",
    "    df = df.drop(columns=datetime_cols)\n",
    "    print(f\"\\n‚úÖ Colonnes datetime supprim√©es : {datetime_cols}\")\n",
    "\n",
    "# ‚úÖ 13. Checking and correcting data types\n",
    "print(\"\\nüîç Types de donn√©es avant transformation :\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Convert ill-typed columns to numeric (if possible)\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    try:\n",
    "        df.loc[:, col] = df[col].astype(float)  # ‚úÖ Use .loc[] to avoid chained copies\n",
    "    except:\n",
    "        pass  # Some columns will remain as text (e.g. 'Label')\n",
    "\n",
    "# ‚úÖ 14. Label column encoding (Attack vs Normal)\n",
    "if 'Label' in df.columns:\n",
    "    le = LabelEncoder()\n",
    "    df.loc[:, 'Label'] = le.fit_transform(df['Label'])  # ‚úÖ Using .loc[]\n",
    "\n",
    "print(\"\\n‚úÖ Distribution des labels apr√®s encodage :\")\n",
    "print(df['Label'].value_counts())\n",
    "\n",
    "# ‚úÖ 15. Standardization of digital features\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "df.loc[:, numerical_cols] = scaler.fit_transform(df[numerical_cols])  # ‚úÖ Using .loc[]\n",
    "\n",
    "print(\"\\n‚úÖ Normalisation des features termin√©e.\")\n",
    "\n",
    "# ‚úÖ 16. Backup of cleaned data\n",
    "df.to_csv(\"cleaned_02-14-2018-2.csv\", index=False)\n",
    "print(\"\\n‚úÖ Fichier cleaned_02-14-2018-2.csv enregistr√© avec succ√®s !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d. Exploration et analyse des donn√©es :\n",
    "- R√©alisez une analyse statistique descriptive.\n",
    "- Visualisez les donn√©es pour identifier les tendances et relations.\n",
    "- Formulez une hypoth√®se initiale √† tester avec les mod√®les de Machine Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Analyse des Donn√©es Nettoy√©es avec Pandas, Seaborn et Matplotlib\n",
    "\n",
    "Ce script effectue une **analyse exploratoire approfondie** des donn√©es apr√®s nettoyage. Il inclut des statistiques descriptives, des visualisations de distribution et de corr√©lation pour mieux comprendre la structure du dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Pr√©requis\n",
    "\n",
    "Avant d'ex√©cuter ce script, assurez-vous d'avoir install√© les biblioth√®ques n√©cessaires avec :\n",
    "\n",
    "```bash\n",
    "pip install pandas numpy seaborn matplotlib\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìÇ Chargement des Donn√©es Nettoy√©es\n",
    "\n",
    "Le script charge un fichier CSV pr√©alablement nettoy√© :\n",
    "\n",
    "```python\n",
    "file_path = \"cleaned_02-14-2018-2.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "```\n",
    "\n",
    "Si le fichier n'est pas dans le m√™me r√©pertoire, ajustez le **chemin du fichier**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 1. Aper√ßu des Premi√®res Lignes\n",
    "\n",
    "Le script affiche un **√©chantillon des premi√®res lignes** du dataset :\n",
    "\n",
    "```python\n",
    "print(\"\\nüìå Aper√ßu des donn√©es nettoy√©es :\")\n",
    "print(df.head())\n",
    "```\n",
    "\n",
    "Cela permet de **v√©rifier rapidement** la structure et le contenu des donn√©es.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 2. Statistiques G√©n√©rales des Donn√©es\n",
    "\n",
    "Les **statistiques descriptives** des variables num√©riques sont affich√©es :\n",
    "\n",
    "```python\n",
    "print(\"\\nüìå Statistiques descriptives des variables num√©riques :\")\n",
    "print(df.describe())\n",
    "```\n",
    "\n",
    "Ce tableau permet d‚Äôobtenir :\n",
    "- **Moyenne (mean)**\n",
    "- **M√©diane (50%)**\n",
    "- **√âcart-type (std)**\n",
    "- **Valeurs minimales et maximales**\n",
    "- **Quartiles (25%, 75%)**\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 3. V√©rification de la Distribution des Labels\n",
    "\n",
    "Le script visualise la **r√©partition des classes** (`Label` = trafic normal ou attaque) :\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.countplot(x=df['Label'], palette=\"viridis\")\n",
    "plt.title(\"R√©partition des classes (Attaque vs Normal)\")\n",
    "plt.xlabel(\"Type de trafic\")\n",
    "plt.ylabel(\"Nombre d'√©chantillons\")\n",
    "plt.xticks(ticks=[0, 1], labels=[\"Normal\", \"Attaque\"])\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "- Affiche **combien d'exemples** appartiennent √† chaque classe.\n",
    "- V√©rifie si le dataset est **√©quilibr√© ou d√©s√©quilibr√©**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 4. V√©rification de la Distribution de Variables Cl√©s\n",
    "\n",
    "Le script g√©n√®re des **histogrammes** pour **analyser la distribution** des principales variables :\n",
    "\n",
    "```python\n",
    "features_to_plot = df.select_dtypes(include=[np.number]).columns[:6]\n",
    "df[features_to_plot].hist(figsize=(12, 8), bins=50, color='royalblue', edgecolor='black')\n",
    "plt.suptitle(\"Histogramme des principales features\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "- S√©lectionne les **6 premi√®res variables num√©riques**.\n",
    "- Affiche leur **r√©partition** sous forme d‚Äô**histogrammes**.\n",
    "- Permet de d√©tecter les **donn√©es asym√©triques ou aberrantes**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 5. Matrice de Corr√©lation des Principales Features\n",
    "\n",
    "Le script affiche une **matrice de corr√©lation** entre les variables :\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(10, 8))\n",
    "corr_matrix = df.corr()\n",
    "sns.heatmap(corr_matrix, annot=False, cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title(\"üîç Matrice de Corr√©lation\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "- Montre **la relation entre les variables**.\n",
    "- Permet d‚Äô**identifier les corr√©lations fortes** (positives ou n√©gatives).\n",
    "- Utile pour d√©tecter les variables **redondantes**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 6. Analyse des Relations entre Variables Cl√©s\n",
    "\n",
    "Un **nuage de points** est g√©n√©r√© pour observer la **relation entre deux variables** :\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=df[features_to_plot[0]], y=df[features_to_plot[1]], hue=df['Label'], alpha=0.5, palette=\"coolwarm\")\n",
    "plt.title(f\"üîç Relation entre {features_to_plot[0]} et {features_to_plot[1]}\")\n",
    "plt.xlabel(features_to_plot[0])\n",
    "plt.ylabel(features_to_plot[1])\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "- **Compare graphiquement deux variables** pour d√©tecter une tendance.\n",
    "- Utilise la **couleur** pour diff√©rencier les classes (`Label`).\n",
    "- Permet de rep√©rer **des patterns entre attaques et trafic normal**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 7. Analyse de la Distribution des Valeurs (Boxplots)\n",
    "\n",
    "Le script g√©n√®re des **boxplots** pour analyser la distribution des variables :\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df[features_to_plot], palette=\"coolwarm\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Boxplot des principales features\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "- Permet de d√©tecter **les valeurs aberrantes** (outliers).\n",
    "- Compare **les distributions des variables** sous forme de quartiles.\n",
    "- Identifie les **variations de dispersion** entre les features.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Conclusion\n",
    "\n",
    "Ce script effectue une **analyse compl√®te** des donn√©es nettoy√©es et permet de :\n",
    "‚úîÔ∏è V√©rifier la **structure et les caract√©ristiques** du dataset  \n",
    "‚úîÔ∏è Observer la **r√©partition des classes (trafic normal vs attaque)**  \n",
    "‚úîÔ∏è Analyser la **distribution des variables num√©riques**  \n",
    "‚úîÔ∏è Identifier les **corr√©lations et tendances cl√©s**  \n",
    "‚úîÔ∏è D√©tecter les **valeurs aberrantes et les d√©s√©quilibres**  \n",
    "\n",
    "---\n",
    "\n",
    "### üéØ R√©sum√© des √âtapes en Code\n",
    "\n",
    "```python\n",
    "# 1. Chargement des donn√©es nettoy√©es\n",
    "df = pd.read_csv(\"cleaned_02-14-2018-2.csv\")\n",
    "\n",
    "# 2. Aper√ßu des donn√©es\n",
    "print(df.head())\n",
    "print(df.describe())\n",
    "\n",
    "# 3. R√©partition des labels (trafic normal vs attaque)\n",
    "sns.countplot(x=df['Label'], palette=\"viridis\")\n",
    "plt.show()\n",
    "\n",
    "# 4. Histogrammes des principales variables\n",
    "df[df.select_dtypes(include=[np.number]).columns[:6]].hist(figsize=(12, 8), bins=50, color='royalblue')\n",
    "plt.show()\n",
    "\n",
    "# 5. Matrice de corr√©lation\n",
    "sns.heatmap(df.corr(), cmap=\"coolwarm\")\n",
    "plt.show()\n",
    "\n",
    "# 6. Relation entre deux variables cl√©s\n",
    "sns.scatterplot(x=df[features_to_plot[0]], y=df[features_to_plot[1]], hue=df['Label'], alpha=0.5, palette=\"coolwarm\")\n",
    "plt.show()\n",
    "\n",
    "# 7. Analyse des valeurs aberrantes avec boxplots\n",
    "sns.boxplot(data=df[features_to_plot], palette=\"coolwarm\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Exploration et analyse des donn√©es termin√©e !\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cleaned data\n",
    "file_path = \"cleaned_02-14-2018-2.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# üîπ 1. Preview of the first lines\n",
    "print(\"\\nüìå Aper√ßu des donn√©es nettoy√©es :\")\n",
    "print(df.head())\n",
    "\n",
    "# üîπ 2. General data statistics\n",
    "print(\"\\nüìå Statistiques descriptives des variables num√©riques :\")\n",
    "print(df.describe())\n",
    "\n",
    "# üîπ 3. Checking label distribution\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.countplot(x=df['Label'], palette=\"viridis\")\n",
    "plt.title(\"R√©partition des classes (Attaque vs Normal)\")\n",
    "plt.xlabel(\"Type de trafic\")\n",
    "plt.ylabel(\"Nombre d'√©chantillons\")\n",
    "plt.xticks(ticks=[0, 1], labels=[\"Normal\", \"Attaque\"])\n",
    "plt.show()\n",
    "\n",
    "# üîπ 4. Checking the distribution of a few key variables\n",
    "features_to_plot = df.select_dtypes(include=[np.number]).columns[:6]  # Selects 6 numerical variables\n",
    "\n",
    "df[features_to_plot].hist(figsize=(12, 8), bins=50, color='royalblue', edgecolor='black')\n",
    "plt.suptitle(\"Histogramme des principales features\")\n",
    "plt.show()\n",
    "\n",
    "# üîπ 5. Correlation matrix of main features\n",
    "plt.figure(figsize=(10, 8))\n",
    "corr_matrix = df.corr()\n",
    "sns.heatmap(corr_matrix, annot=False, cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title(\"üîç Matrice de Corr√©lation\")\n",
    "plt.show()\n",
    "\n",
    "# üîπ 6. Analysis of relationships between key variables (e.g. duration vs. throughput)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x=df[features_to_plot[0]], y=df[features_to_plot[1]], hue=df['Label'], alpha=0.5, palette=\"coolwarm\")\n",
    "plt.title(f\"üîç Relation entre {features_to_plot[0]} et {features_to_plot[1]}\")\n",
    "plt.xlabel(features_to_plot[0])\n",
    "plt.ylabel(features_to_plot[1])\n",
    "plt.show()\n",
    "\n",
    "# üîπ 7. Boxplot to analyze the distribution of values\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df[features_to_plot], palette=\"coolwarm\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Boxplot des principales features\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Exploration et analyse des donn√©es termin√©e !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e. Mod√©lisation des donn√©es :\n",
    "\n",
    "- S√©lectionnez un mod√®le de Machine Learning adapt√©.\n",
    "- Divisez les donn√©es en deux parties : Training et Test.\n",
    "- Entra√Ænez, optimisez et √©valuez les performances du mod√®le.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Entra√Ænement et Optimisation d'un Mod√®le de Machine Learning\n",
    "\n",
    "Ce script effectue **l'entra√Ænement, l'√©valuation et l'optimisation** d'un mod√®le de Machine Learning pour **classifier le trafic r√©seau** en \"Normal\" ou \"Attaque\" √† l'aide de l'algorithme **Random Forest**. \n",
    "\n",
    "Il comprend :\n",
    "‚úîÔ∏è **Pr√©paration des donn√©es**  \n",
    "‚úîÔ∏è **S√©lection et entra√Ænement du mod√®le**  \n",
    "‚úîÔ∏è **√âvaluation des performances**  \n",
    "‚úîÔ∏è **Optimisation des hyperparam√®tres**  \n",
    "‚úîÔ∏è **Sauvegarde du mod√®le final**  \n",
    "\n",
    "---\n",
    "\n",
    "## üìå Pr√©requis\n",
    "\n",
    "Assurez-vous d'avoir install√© les biblioth√®ques n√©cessaires avec :\n",
    "\n",
    "```bash\n",
    "pip install pandas numpy seaborn matplotlib scikit-learn joblib\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 1. Chargement des Donn√©es Nettoy√©es\n",
    "\n",
    "Le dataset pr√©alablement nettoy√© est charg√© dans un **DataFrame** :\n",
    "\n",
    "```python\n",
    "file_path = \"cleaned_02-14-2018.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "```\n",
    "\n",
    "**üìå Note** : Assurez-vous que le fichier est dans le bon r√©pertoire ou ajustez le **chemin d'acc√®s**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 2. S√©paration des Features (`X`) et de la Variable Cible (`y`)\n",
    "\n",
    "Les **features** (`X`) et la **cible** (`y`) sont s√©par√©es :\n",
    "\n",
    "```python\n",
    "X = df.drop(columns=['Label'])  # Features\n",
    "y = df['Label']  # Target\n",
    "```\n",
    "\n",
    "- `X` contient les variables explicatives.\n",
    "- `y` est la variable cible (`0 = Normal`, `1 = Attaque`).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 3. Division en Jeu d'Entra√Ænement et de Test (80/20)\n",
    "\n",
    "Le dataset est divis√© en **80% d'entra√Ænement** et **20% de test** :\n",
    "\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "```\n",
    "\n",
    "- **`stratify=y`** assure une distribution √©quilibr√©e des classes dans les ensembles d'entra√Ænement et de test.\n",
    "- **Affichage des tailles des ensembles** :\n",
    "\n",
    "```python\n",
    "print(f\"- Entra√Ænement : {X_train.shape[0]} √©chantillons\")\n",
    "print(f\"- Test : {X_test.shape[0]} √©chantillons\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 4. S√©lection du Mod√®le de Machine Learning\n",
    "\n",
    "Le **mod√®le Random Forest** est choisi pour sa robustesse et sa capacit√© √† g√©rer des donn√©es complexes :\n",
    "\n",
    "```python\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "```\n",
    "\n",
    "- **`n_estimators=100`** : Utilise **100 arbres** dans la for√™t.\n",
    "- **`n_jobs=-1`** : Utilise **tous les c≈ìurs disponibles** du processeur.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 5. Entra√Ænement du Mod√®le\n",
    "\n",
    "Le mod√®le est entra√Æn√© sur l'ensemble d'apprentissage :\n",
    "\n",
    "```python\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "Une confirmation est affich√©e apr√®s l'entra√Ænement :\n",
    "\n",
    "```python\n",
    "print(\"\\n‚úÖ Mod√®le entra√Æn√© avec succ√®s !\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 6. Pr√©dictions sur le Jeu de Test\n",
    "\n",
    "Les pr√©dictions sont effectu√©es sur les **donn√©es de test** :\n",
    "\n",
    "```python\n",
    "y_pred = model.predict(X_test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 7. √âvaluation des Performances du Mod√®le\n",
    "\n",
    "L'**exactitude (accuracy)** du mod√®le est calcul√©e :\n",
    "\n",
    "```python\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nüìä Pr√©cision du mod√®le : {accuracy:.4f}\")\n",
    "```\n",
    "\n",
    "Un **rapport de classification** d√©taill√© est affich√© :\n",
    "\n",
    "```python\n",
    "print(classification_report(y_test, y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 8. Matrice de Confusion\n",
    "\n",
    "Une **matrice de confusion** est g√©n√©r√©e pour √©valuer les erreurs du mod√®le :\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[\"Normal\", \"Attaque\"], yticklabels=[\"Normal\", \"Attaque\"])\n",
    "plt.xlabel(\"Pr√©dictions\")\n",
    "plt.ylabel(\"R√©el\")\n",
    "plt.title(\"üîç Matrice de Confusion\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "Cela permet d'identifier :\n",
    "- **Les vrais positifs et n√©gatifs** (pr√©dictions correctes).\n",
    "- **Les faux positifs et n√©gatifs** (erreurs de classification).\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 9. Optimisation du Mod√®le avec GridSearchCV\n",
    "\n",
    "Le **tuning des hyperparam√®tres** est effectu√© avec `GridSearchCV` :\n",
    "\n",
    "```python\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42, n_jobs=-1), \n",
    "                           param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "Les **meilleurs hyperparam√®tres** sont affich√©s :\n",
    "\n",
    "```python\n",
    "print(f\"\\n‚úÖ Meilleurs hyperparam√®tres trouv√©s : {grid_search.best_params_}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 10. R√©entra√Ænement avec les Meilleurs Param√®tres\n",
    "\n",
    "Le **meilleur mod√®le** issu de GridSearch est r√©cup√©r√© et r√©entra√Æn√© :\n",
    "\n",
    "```python\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 11. Nouvelle √âvaluation du Mod√®le Optimis√©\n",
    "\n",
    "Le mod√®le optimis√© est test√© √† nouveau :\n",
    "\n",
    "```python\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "print(f\"\\nüìä Pr√©cision du mod√®le optimis√© : {accuracy_best:.4f}\")\n",
    "```\n",
    "\n",
    "Un **nouveau rapport de classification** est affich√© :\n",
    "\n",
    "```python\n",
    "print(\"\\nüìå Rapport de classification apr√®s optimisation :\")\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 12. Sauvegarde du Mod√®le Entra√Æn√©\n",
    "\n",
    "Le mod√®le final est sauvegard√© au format `.pkl` avec `joblib` :\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "joblib.dump(best_model, \"final_model.pkl\")\n",
    "```\n",
    "\n",
    "Une confirmation est affich√©e :\n",
    "\n",
    "```python\n",
    "print(\"\\n‚úÖ Mod√®le sauvegard√© sous 'final_model.pkl'\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ R√©sum√© des √âtapes\n",
    "\n",
    "```python\n",
    "# 1. Chargement des donn√©es nettoy√©es\n",
    "df = pd.read_csv(\"cleaned_02-14-2018.csv\")\n",
    "\n",
    "# 2. S√©paration des features et de la cible\n",
    "X = df.drop(columns=['Label'])\n",
    "y = df['Label']\n",
    "\n",
    "# 3. Division des donn√©es en train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 4. Mod√®le Random Forest\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. Pr√©dictions et √©valuation\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# 6. Matrice de confusion\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "plt.show()\n",
    "\n",
    "# 7. Optimisation avec GridSearchCV\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "                           param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# 8. R√©√©valuation du mod√®le optimis√©\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "print(f\"Optimized Accuracy: {accuracy_score(y_test, y_pred_best):.4f}\")\n",
    "\n",
    "# 9. Sauvegarde du mod√®le\n",
    "joblib.dump(best_model, \"final_model.pkl\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ 2. Separate features (X) and target variable (y)\n",
    "X = df.drop(columns=['Label'])  # Features\n",
    "y = df['Label']  # Target\n",
    "\n",
    "# ‚úÖ 3. Division into training (80%) and test (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"\\nüìå Taille des jeux de donn√©es :\")\n",
    "print(f\"- Entra√Ænement : {X_train.shape[0]} √©chantillons\")\n",
    "print(f\"- Test : {X_test.shape[0]} √©chantillons\")\n",
    "\n",
    "# ‚úÖ 4. Machine Learning model selection (Random Forest)\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# ‚úÖ 5. Model drive\n",
    "print(\"\\nüöÄ Entra√Ænement du mod√®le...\")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"\\n‚úÖ Mod√®le entra√Æn√© avec succ√®s !\")\n",
    "\n",
    "# ‚úÖ 6. Test set predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# ‚úÖ 7. Model performance evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nüìä Pr√©cision du mod√®le : {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nüìå Rapport de classification :\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ‚úÖ 8. Confusion matrix\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues', xticklabels=[\"Normal\", \"Attaque\"], yticklabels=[\"Normal\", \"Attaque\"])\n",
    "plt.xlabel(\"Pr√©dictions\")\n",
    "plt.ylabel(\"R√©el\")\n",
    "plt.title(\"üîç Matrice de Confusion\")\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ 9. Model optimization with GridSearchCV (Hyperparameter Tuning)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42, n_jobs=-1), param_grid, cv=3, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"\\n‚úÖ Meilleurs hyperparam√®tres trouv√©s : {grid_search.best_params_}\")\n",
    "\n",
    "# ‚úÖ 10. Retraining with the best parameters\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# ‚úÖ 11. New evaluation of the optimized model\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "print(f\"\\nüìä Pr√©cision du mod√®le optimis√© : {accuracy_best:.4f}\")\n",
    "\n",
    "print(\"\\nüìå Rapport de classification apr√®s optimisation :\")\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "\n",
    "# ‚úÖ 12. Saving the trained model\n",
    "import joblib\n",
    "joblib.dump(best_model, \"final_model.pkl\")\n",
    "print(\"\\n‚úÖ Mod√®le sauvegard√© sous 'final_model.pkl'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f. Interpr√©tation des r√©sultats :\n",
    "- Analysez les r√©sultats obtenus et validez (ou invalidez) votre hypoth√®se.\n",
    "- Tirez des conclusions et proposez des insights m√©tiers bas√©s sur votre analyse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä √âvaluation et Interpr√©tation du Mod√®le Optimis√©\n",
    "\n",
    "Ce script analyse les **performances** du mod√®le de classification entra√Æn√© et optimis√©. Il g√©n√®re des **m√©triques cl√©s**, des **visualisations** (matrice de confusion, courbes ROC et pr√©cision-rappel) et met en √©vidence les **features les plus importantes**. Enfin, il propose une **interpr√©tation m√©tier** des r√©sultats.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Pr√©requis\n",
    "\n",
    "Avant d'ex√©cuter ce script, installez les biblioth√®ques n√©cessaires avec :\n",
    "\n",
    "```bash\n",
    "pip install pandas numpy seaborn matplotlib scikit-learn\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 1. Rapport de Classification du Mod√®le Optimis√©\n",
    "\n",
    "Le rapport de classification fournit un **r√©sum√© d√©taill√©** des performances du mod√®le :\n",
    "\n",
    "```python\n",
    "print(\"\\nüìå Rapport de classification du mod√®le optimis√© :\")\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "```\n",
    "\n",
    "- Affiche **pr√©cision (precision), rappel (recall), F1-score** et **support** pour chaque classe.\n",
    "- Utile pour voir si le mod√®le favorise une **classe** plus que l‚Äôautre.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 2. Matrice de Confusion (Analyse des Erreurs)\n",
    "\n",
    "La **matrice de confusion** visualise les **pr√©dictions correctes et erron√©es** du mod√®le :\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(6, 4))\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_best)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[\"Normal\", \"Attaque\"], yticklabels=[\"Normal\", \"Attaque\"])\n",
    "plt.xlabel(\"Pr√©dictions\")\n",
    "plt.ylabel(\"R√©el\")\n",
    "plt.title(\"üîç Matrice de Confusion - Mod√®le Optimis√©\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "- **Les valeurs sur la diagonale** (haut gauche et bas droit) repr√©sentent les **pr√©dictions correctes**.\n",
    "- **Les valeurs hors diagonale** montrent les **erreurs de classification**.\n",
    "- Permet d‚Äôanalyser si le mod√®le **manque trop d‚Äôattaques (faux n√©gatifs)** ou **pr√©dit trop de faux positifs**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 3. Analyse des M√©triques Cl√©s\n",
    "\n",
    "Les m√©triques sont extraites de la matrice de confusion :\n",
    "\n",
    "```python\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "accuracy = (tp + tn) / (tn + fp + fn + tp)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f\"\\nüìä Pr√©cision (Precision) : {precision:.4f}\")\n",
    "print(f\"üìä Rappel (Recall) : {recall:.4f}\")\n",
    "print(f\"üìä Score F1 : {f1_score:.4f}\")\n",
    "print(f\"üìä Exactitude (Accuracy) : {accuracy:.4f}\")\n",
    "```\n",
    "\n",
    "- **Pr√©cision (Precision)** : Taux de pr√©dictions correctes parmi les **attaques d√©tect√©es**.\n",
    "- **Rappel (Recall)** : Taux de **vraies attaques d√©tect√©es** sur l‚Äôensemble des attaques r√©elles.\n",
    "- **F1-score** : Moyenne harmonique entre pr√©cision et rappel (**√©quilibre entre faux positifs et faux n√©gatifs**).\n",
    "- **Exactitude (Accuracy)** : Proportion d‚Äô√©chantillons **correctement classifi√©s**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 4. Courbe ROC et AUC\n",
    "\n",
    "La **courbe ROC** permet de visualiser la capacit√© du mod√®le √† **distinguer les classes** :\n",
    "\n",
    "```python\n",
    "y_prob = best_model.predict_proba(X_test)[:, 1]  # Probabilit√© de la classe \"Attaque\"\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.4f}')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonale de hasard\n",
    "plt.xlabel(\"Taux de Faux Positifs (FPR)\")\n",
    "plt.ylabel(\"Taux de Vrais Positifs (TPR)\")\n",
    "plt.title(\"üîç Courbe ROC\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "- **FPR (False Positive Rate)** : Proportion de **faux positifs**.\n",
    "- **TPR (True Positive Rate)** : Proportion de **vrais positifs**.\n",
    "- **AUC (Area Under Curve)** : Score global de la capacit√© du mod√®le √† s√©parer les classes.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 5. Courbe Pr√©cision-Rappel\n",
    "\n",
    "La **courbe pr√©cision-rappel** est utile lorsque les classes sont **d√©s√©quilibr√©es** :\n",
    "\n",
    "```python\n",
    "precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(recall_vals, precision_vals, color='purple', lw=2)\n",
    "plt.xlabel(\"Rappel\")\n",
    "plt.ylabel(\"Pr√©cision\")\n",
    "plt.title(\"üîç Courbe Pr√©cision-Rappel\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "- Permet d‚Äôanalyser comment le **rappel** √©volue en fonction de la **pr√©cision**.\n",
    "- Si la **pr√©cision chute rapidement**, cela signifie un **nombre √©lev√© de faux positifs**.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 6. Analyse des Features les Plus Importantes\n",
    "\n",
    "Le mod√®le **Random Forest** permet d‚Äôidentifier les **variables les plus influentes** :\n",
    "\n",
    "```python\n",
    "feature_importances = best_model.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importances)[-10:]  # Top 10 features\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(range(len(sorted_idx)), feature_importances[sorted_idx], align=\"center\")\n",
    "plt.yticks(range(len(sorted_idx)), [X.columns[i] for i in sorted_idx])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"üìä Top 10 des Features les Plus Importantes\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "- Met en √©vidence les **10 variables les plus d√©terminantes** pour la pr√©diction.\n",
    "- Peut √™tre utilis√© pour **s√©lectionner les meilleures features** et am√©liorer le mod√®le.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ 7. Interpr√©tation des R√©sultats\n",
    "\n",
    "Le mod√®le est interpr√©t√© en fonction des **m√©triques calcul√©es** :\n",
    "\n",
    "```python\n",
    "print(\"\\nüìå üîç Interpr√©tation des r√©sultats üîç\")\n",
    "\n",
    "if accuracy > 0.90:\n",
    "    print(\"‚úÖ Le mod√®le a une tr√®s bonne performance globale avec une pr√©cision √©lev√©e.\")\n",
    "elif accuracy > 0.80:\n",
    "    print(\"‚úÖ Le mod√®le a une bonne pr√©cision, mais pourrait √™tre am√©lior√©.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Le mod√®le a une pr√©cision faible. Il pourrait n√©cessiter un meilleur pr√©traitement ou un autre algorithme.\")\n",
    "\n",
    "print(\"\\nüìå Insights m√©tiers :\")\n",
    "if precision < 0.80:\n",
    "    print(\"- ‚ö†Ô∏è Le taux de faux positifs est √©lev√©, ce qui signifie que certaines connexions normales sont class√©es comme des attaques.\")\n",
    "if recall < 0.80:\n",
    "    print(\"- ‚ö†Ô∏è Le mod√®le manque certaines attaques (faux n√©gatifs), ce qui peut poser un risque en cybers√©curit√©.\")\n",
    "if f1_score > 0.85:\n",
    "    print(\"- ‚úÖ Le compromis entre pr√©cision et rappel est bon, ce qui signifie que le mod√®le est fiable pour d√©tecter les intrusions.\")\n",
    "if feature_importances.max() > 0.1:\n",
    "    print(\"- ‚úÖ Certaines features sont tr√®s influentes. Un expert en cybers√©curit√© peut analyser leur impact.\")\n",
    "\n",
    "print(\"\\n‚úÖ Analyse termin√©e !\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ 1. Display of optimized model results\n",
    "print(\"\\nüìå Rapport de classification du mod√®le optimis√© :\")\n",
    "print(classification_report(y_test, y_pred_best))\n",
    "\n",
    "# ‚úÖ 2. Confusion matrix (error analysis)\n",
    "plt.figure(figsize=(6, 4))\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_best)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Normal\", \"Attaque\"], yticklabels=[\"Normal\", \"Attaque\"])\n",
    "plt.xlabel(\"Pr√©dictions\")\n",
    "plt.ylabel(\"R√©el\")\n",
    "plt.title(\"üîç Matrice de Confusion - Mod√®le Optimis√©\")\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ 3. Analysis of key metrics\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "accuracy = (tp + tn) / (tn + fp + fn + tp)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f\"\\nüìä Pr√©cision (Precision) : {precision:.4f}\")\n",
    "print(f\"üìä Rappel (Recall) : {recall:.4f}\")\n",
    "print(f\"üìä Score F1 : {f1_score:.4f}\")\n",
    "print(f\"üìä Exactitude (Accuracy) : {accuracy:.4f}\")\n",
    "\n",
    "# ‚úÖ 4. ROC and AUC curves\n",
    "y_prob = best_model.predict_proba(X_test)[:, 1]  # Probabilities for the ‚ÄúAttack‚Äù class\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.4f}')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonal of chance\n",
    "plt.xlabel(\"Taux de Faux Positifs (FPR)\")\n",
    "plt.ylabel(\"Taux de Vrais Positifs (TPR)\")\n",
    "plt.title(\"üîç Courbe ROC\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ 5. Precision-Recall curve\n",
    "precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(recall_vals, precision_vals, color='purple', lw=2)\n",
    "plt.xlabel(\"Rappel\")\n",
    "plt.ylabel(\"Pr√©cision\")\n",
    "plt.title(\"üîç Courbe Pr√©cision-Rappel\")\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ 6. Analysis of the most important features\n",
    "feature_importances = best_model.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importances)[-10:]  # Top 10 features\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(range(len(sorted_idx)), feature_importances[sorted_idx], align=\"center\")\n",
    "plt.yticks(range(len(sorted_idx)), [X.columns[i] for i in sorted_idx])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"üìä Top 10 des Features les Plus Importantes\")\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ 7. Interpretation of results\n",
    "print(\"\\nüìå üîç Interpr√©tation des r√©sultats üîç\")\n",
    "\n",
    "if accuracy > 0.90:\n",
    "    print(\"‚úÖ Le mod√®le a une tr√®s bonne performance globale avec une pr√©cision √©lev√©e.\")\n",
    "elif accuracy > 0.80:\n",
    "    print(\"‚úÖ Le mod√®le a une bonne pr√©cision, mais pourrait √™tre am√©lior√©.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Le mod√®le a une pr√©cision faible. Il pourrait n√©cessiter un meilleur pr√©traitement ou un autre algorithme.\")\n",
    "\n",
    "print(\"\\nüìå Insights m√©tiers :\")\n",
    "if precision < 0.80:\n",
    "    print(\"- ‚ö†Ô∏è Le taux de faux positifs est √©lev√©, ce qui signifie que certaines connexions normales sont class√©es comme des attaques.\")\n",
    "if recall < 0.80:\n",
    "    print(\"- ‚ö†Ô∏è Le mod√®le manque certaines attaques (faux n√©gatifs), ce qui peut poser un risque en cybers√©curit√©.\")\n",
    "if f1_score > 0.85:\n",
    "    print(\"- ‚úÖ Le compromis entre pr√©cision et rappel est bon, ce qui signifie que le mod√®le est fiable pour d√©tecter les intrusions.\")\n",
    "if feature_importances.max() > 0.1:\n",
    "    print(\"- ‚úÖ Certaines features sont tr√®s influentes. Un expert en cybers√©curit√© peut analyser leur impact.\")\n",
    "\n",
    "print(\"\\n‚úÖ Analyse termin√©e !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g. Communication des r√©sultats :\n",
    "Cr√©ez des visualisations claires et pertinentes pour pr√©senter vos r√©sultats.\n",
    "Formulez des recommandations concr√®tes bas√©es sur vos analyses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ 1. R√©capitulatif des performances\n",
    "accuracy_val = accuracy_score(y_test, y_pred_best)\n",
    "precision_val = precision_score(y_test, y_pred_best)\n",
    "recall_val = recall_score(y_test, y_pred_best)\n",
    "f1_val = f1_score(y_test, y_pred_best)\n",
    "\n",
    "print(\"\\nüìä R√©capitulatif des performances du mod√®le :\")\n",
    "print(f\"‚úÖ Pr√©cision (Accuracy) : {accuracy:.4f}\")\n",
    "print(f\"‚úÖ Pr√©cision (Precision) : {precision:.4f}\")\n",
    "print(f\"‚úÖ Rappel (Recall) : {recall:.4f}\")\n",
    "print(f\"‚úÖ Score F1 : {f1:.4f}\")\n",
    "\n",
    "# ‚úÖ 2. Matrice de confusion avec annotations\n",
    "plt.figure(figsize=(6, 4))\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_best)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"Normal\", \"Attaque\"], yticklabels=[\"Normal\", \"Attaque\"])\n",
    "plt.xlabel(\"Pr√©dictions\")\n",
    "plt.ylabel(\"R√©el\")\n",
    "plt.title(\"üîç Matrice de Confusion - R√©sultats finaux\")\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ 3. Courbe ROC et AUC\n",
    "y_prob = best_model.predict_proba(X_test)[:, 1]  # Probabilit√©s pour la classe \"Attaque\"\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'AUC = {roc_auc:.4f}')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # Diagonale de hasard\n",
    "plt.xlabel(\"Taux de Faux Positifs (FPR)\")\n",
    "plt.ylabel(\"Taux de Vrais Positifs (TPR)\")\n",
    "plt.title(\"üîç Courbe ROC\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ 4. Courbe Precision-Recall\n",
    "precision_vals, recall_vals, _ = precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(recall_vals, precision_vals, color='purple', lw=2)\n",
    "plt.xlabel(\"Rappel\")\n",
    "plt.ylabel(\"Pr√©cision\")\n",
    "plt.title(\"üîç Courbe Pr√©cision-Rappel\")\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ 5. Importance des Features\n",
    "feature_importances = best_model.feature_importances_\n",
    "sorted_idx = np.argsort(feature_importances)[-10:]  # Top 10 features\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.barh(range(len(sorted_idx)), feature_importances[sorted_idx], align=\"center\")\n",
    "plt.yticks(range(len(sorted_idx)), [X.columns[i] for i in sorted_idx])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"üìä Top 10 des Features les Plus Importantes\")\n",
    "plt.show()\n",
    "\n",
    "# ‚úÖ 6. Formulation des recommandations\n",
    "print(\"\\nüìå üîç Recommandations bas√©es sur les r√©sultats üîç\")\n",
    "\n",
    "if accuracy > 0.90:\n",
    "    print(\"‚úÖ Le mod√®le est fiable avec une haute pr√©cision. Il peut √™tre utilis√© pour une premi√®re d√©tection des menaces.\")\n",
    "elif accuracy > 0.80:\n",
    "    print(\"‚úÖ Le mod√®le est performant mais peut √™tre optimis√© (ex: am√©lioration du dataset, hyperparam√®tres).\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Le mod√®le n'est pas encore optimal. Il faudrait explorer d'autres algorithmes ou am√©liorer la qualit√© des donn√©es.\")\n",
    "\n",
    "if precision < 0.80:\n",
    "    print(\"- ‚ö†Ô∏è Attention : Le taux de faux positifs est √©lev√©, ce qui pourrait g√©n√©rer des alertes inutiles.\")\n",
    "if recall < 0.80:\n",
    "    print(\"- ‚ö†Ô∏è Attention : Le mod√®le ne d√©tecte pas toutes les attaques, un ajustement est n√©cessaire.\")\n",
    "\n",
    "print(\"\\nüìå Actions recommand√©es :\")\n",
    "print(\"- Tester d'autres mod√®les (XGBoost, Deep Learning).\")\n",
    "print(\"- Am√©liorer les donn√©es en ajoutant d'autres logs r√©seau.\")\n",
    "print(\"- Ajuster les hyperparam√®tres du mod√®le pour un meilleur rappel.\")\n",
    "print(\"- Impl√©menter une d√©tection en temps r√©el avec ce mod√®le.\")\n",
    "\n",
    "print(\"\\n‚úÖ Communication des r√©sultats termin√©e !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
